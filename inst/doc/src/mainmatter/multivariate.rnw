\documentclass{article}
\input{../config/commontex}

\title{Multivariate analyses}
\author{Lobry, J.R.}

\begin{document}
\SweaveInput{../config/commonrnw.rnw}
\maketitle
% BEGIN - DO NOT REMOVE THIS LINE

\section{Correspondence analysis}

This is the most popular multivariate data analysis technique for amino-acid
and codon count tables, its application, however, is not without pitfalls \cite{misuse}.
Its primary goal is to transform a table of counts
into a graphical display, in which each gene (or protein) and each codon (or amino-acid)
is depicted as a point. Correspondence analysis (CA) may be defined as a special 
case of principal components analysis (PCA) with a different underlying metrics.
The interest of the metrics in CA, that is the way we measure the distance between
two individuals, is illustrated bellow with a very simple example (Table \ref{toyaa} inspired from \cite{CG}) with only
three proteins having only three amino-acids, so that we can represent exactly
on a map the consequences of the metric choice.

<<toyaa,eval=T>>=
data(toyaa)
toyaa
@

<<toyaa, fig = FALSE, echo = FALSE,eval=T>>=
print(xtable(toyaa, digits = rep(0,4), caption = "A very simple example of amino-acid counts in three proteins to be loaded with \\texttt{data(toyaa).}", label = "toyaa"), 
file = "../tables/toyaa.tex")
@
\input{../tables/toyaa.tex}

Let's first use the regular Euclidian metrics between two proteins $i$ and $i'$,
\begin{equation}
d^2(i,i') = \sum_{j=1}^{J}(n_{ij} - n_{i'j})^2
\label{euclidian}
\end{equation}
to visualize this small data set:

<<euclidian, fig = TRUE,eval=T>>=
library(ade4)
pco <- dudi.pco(dist(toyaa), scann = F, nf = 2)
myplot <- function( res, ... )
{
  plot(res$li[ , 1], res$li[ , 2], ...)
  text(x = res$li[ , 1], y = res$li[ , 2], labels = 1:3, pos = ifelse(res$li[ , 2] < 0, 1, 3))
  perm <- c(3, 1, 2)
  lines( c(res$li[ , 1], res$li[perm, 1]), c(res$li[ , 2], res$li[perm, 2]))
}
myplot(pco, main = "Euclidian distance", asp = 1, pch = 19, xlab = "", ylab = "", las = 1)
@

From this point of view, the first individual is far away from the two others. But
thinking about it, this is a rather trivial effect of protein size:

<<protsize,eval=T>>=
rowSums(toyaa)
@

With \Sexpr{ifelse(exists("toyaa"), rowSums(toyaa)[1], "???")} amino-acids, the first protein is two times bigger 
than the others so that when computing the Euclidian distance (\ref{euclidian}) its $n_{ij}$ entries
are on average bigger, sending it away from the others.
To get rid of this trivial effect, the first
obvious idea is to divide counts by protein lengths so as to work with 
\emph{protein profiles}. The corresponding distance is,

\begin{equation}
d^2(i,i') = \sum_{j=1}^{J}(\frac{n_{ij}}{n_{i\bullet}} - \frac{n_{i'j}}{n_{i'\bullet}})^2
\label{euclprof}
\end{equation}

where $n_{i\bullet}$ and $n_{i'\bullet}$ are the total number of amino-acids
in protein $i$ and $i'$, respectively.

<<profile, fig = TRUE,eval=T>>=
profile <- toyaa/rowSums(toyaa)
profile
dudi.pco(dist(profile), scann = F, nf = 2) -> pco1
myplot(pco1, main = "Euclidian distance on protein profiles", asp = 1, pch = 19, xlab = "", ylab = "",
  ylim = range(pco1$li[ , 2])*1.2)
@

The pattern is now completely different with the three protein equally spaced.
This is normal because in terms of relative amino-acid composition they are
all differing two-by-two by $5\%$ at the level of two amino-acids only. We
have clearly removed the trivial protein size effect, but this is still not completely
satisfactory. The proteins are differing by $5\%$ for all amino-acids but the situation 
is somewhat different for \texttt{Cys} because this amino-acid is very rare.
A difference of $5\%$ for a rare amino-acid has not the same significance than
a difference of $5\%$ for a common amino-acid such as \texttt{Ala} in our
example. To cope with this, CA make use of a variance-standardizing
technique to compensate for the larger variance in high frequencies and the 
smaller variance in low frequencies. This is achieved with the use of the 
\emph{chi-square distance} ($\chi^2$) which differs from the previous Euclidean distance 
on profiles (\ref{euclprof}) in that each square is weighted by the inverse of 
the frequency corresponding to each term,

\begin{equation}
d^2(i,i') = n_{\bullet\bullet}\sum_{j=1}^{J}\frac{1}{n_{{\bullet}j}}(\frac{n_{ij}}{n_{i\bullet}} - \frac{n_{i'j}}{n_{i'\bullet}})^2
\label{chi}
\end{equation}

where $n_{{\bullet}j}$ is the total number of amino-acid of kind $j$ and
$n_{\bullet\bullet}$ the total number of amino-acids. With this point
of view, the map is now like this:

<<afc, fig = TRUE,eval=T>>=
coa <- dudi.coa(toyaa, scann = FALSE, nf = 2)
myplot(coa, main = expression(paste(chi^2," distance")), 
  asp = 1, pch = 19, xlab = "", ylab = "")
@

The pattern is completely different with now protein number 3 which is far away from
the others because it is enriched in the rare amino-acid \texttt{Cys} as compared to
others.

The purpose of this small example was to demonstrates that the metric choice
is not without dramatic effects on the visualisation of data. Depending on your
objectives, you may agree or disagree with the $\chi^2$ metric choice, that's
not a problem, the important point is that you should be aware that there is
an underlying model there, \textit{chacun a son go{\^u}t} ou 
\textit{chacun {\`a} son go{\^u}t}, it's up to you.

Now, if you agree with the  $\chi^2$ metric choice, there's a nice
representation that may help you for the interpretation of results. 
This is a kind of "biplot" representation in which the lines and
columns of the dataset are simultaneously represented, in the
right way, that is as a graphical \textit{translation} of a 
mathematical theorem, but let's see how does it look like in practice: 

<<scatter, fig = TRUE,eval=T>>=
scatter(coa, clab.col = 0.8, clab.row = 0.8, posi = "none")
@

What is obvious is that the Cys content has a major effect on protein
variability here, no scoop. Please note how the information is well
summarised here: protein number 3 differs because it's enriched in
in Cys ; protein number 1 and 2 are almost the same but there is a
small trend protein number 1 to be enriched in Ala. As compared to
to table \ref{toyaa} this graph is of poor information here, so let's
try a more big-rooom-sized example (with $20$ columns so as to
illustrate the dimension reduction technique).

Data are from \cite{lobrygautier}, a sample of the proteome of
\textit{Escherichia coli}. According to the title of this paper,
the most important factor for the between-protein variability is
hydrophilic - hydrophobic gradient. Let's try to reproduce this
assertion :

<<lobrygautier,fig=T,eval=T>>=
download.file(url="ftp://pbil.univ-lyon1.fr/pub/datasets/NAR94/data.txt", destfile = "data.txt")

ec <- read.table(file = "data.txt", header = TRUE, 
    row.names = 1)
    
ec.coa <- dudi.coa(ec, scann = FALSE, nf = 1)
F1 <- ec.coa$li[,1]
hist(F1, proba = TRUE, xlab = "First factor for amino-acid variability",
col = grey(0.8), border = grey(0.5), las = 1, ylim = c(0,6),
        main="Protein distribution on first factor")
lines(density(F1, adjust = 0.5), lwd = 2)
@

There is clearly a bimodal distribution of proteins on the first factor. What are the
the amino-acid coordinates on this factor?

<<lobrygautier2,fig=T,height=5, eval=T>>=
aacoo <- ec.coa$co[ , 1]
names(aacoo) <- rownames(ec.coa$co)
aacoo <- sort( aacoo)
dotchart(aacoo, pch = 19, xlab = "Coordinate on first factor",
main = "Amino acid coordinates on first factor")
@

Aliphatic and aromatic amino-acids have positive values while charged amino-acids
have negative values\footnote{The physico-chemical classes for amino acids are
given in the component \texttt{AA.PROPERTY} of the \texttt{SEQINR.UTIL}
object.}. Let's try to compute the GRAVY score (\textit{i.e.} the Kyte and Doolittle 
hydropathic index\cite{KD}) of our proteins to compare this with their coordinates 
on the first factor. We need first the amino-acid \emph{relatives} frequencies in the
proteins, for this we divide the all the amino-acid counts by the total by row:

<<lobrygautier3,fig=F, eval=T>>=
ecfr <- ec/rowSums(ec)
ecfr[1:5, 1:5]
@

We need also the coefficients corresponding to the GRAVY score:

<<lobrygautier4,fig=F, eval=T>>=
gravy <- read.table(file ="ftp://pbil.univ-lyon1.fr/pub/datasets/NAR94/gravy.txt")
gravy[1:5, ]
coef <- gravy$V2
@

The coefficient are given in the alphabetical order of the three letter code for
the amino acids, that is in a different order than in the object \texttt{ecfr}:

<<lobrygautier5,fig=F, eval=T>>=
names(ecfr)
@

We then re-order the columns of the data set and check that everthing is OK:

<<lobrygautier6,fig=F, eval=T>>=
ecfr <- ecfr[ , order(names(ecfr))]
ecfr[1:5,1:5]
all(names(ecfr) == tolower(as.character(gravy$V1)))
@

Now, thanks to R build-in matrix multiplication, it's only one line to compute
the GRAVY score:

<<lobrygautier5,fig=T,eval=T>>=
as.matrix(ecfr) %*% coef -> gscores
plot(gscores,F1,xlab="GRAVY Score", ylab="F1 Score",las=1,main="The first factor is protein hydrophaty")
@

The proteins with high GRAVY scores are integral membrane proteins, and those
with low scores are cytoplasmic proteins. Now, suppose that we want to adjust
a mixture of two normal distributions to get an estimate of the proportion of
cytoplasmic and integral membrane proteins. We first have a look on the predefined
distributions (Table \ref{dpqr}), but there is apparently not an out of the box
solution.
<<dpqrtable, fig = FALSE, results = hide, echo = FALSE, eval=T>>=
noms <- ls("package:stats")
startwith <- function(nom, char)
{
  substr(nom, 1, 1) == char
}
(dnoms <- noms[sapply(noms, startwith, "d")])
(pnoms <- noms[sapply(noms, startwith, "p")])
(qnoms <- noms[sapply(noms, startwith, "q")])
(rnoms <- noms[sapply(noms, startwith, "r")])
suffix <- function(nom)
{
  substr(nom, 2, nchar(nom))
}
(dnoms <- sapply(dnoms, suffix, USE.NAMES = FALSE))
(pnoms <- sapply(pnoms, suffix, USE.NAMES = FALSE))
(qnoms <- sapply(qnoms, suffix, USE.NAMES = FALSE))
(rnoms <- sapply(rnoms, suffix, USE.NAMES = FALSE))
(dpqr <- dnoms[dnoms %in% pnoms & dnoms %in% qnoms & dnoms %in% rnoms])
(ddpqr <-  paste("d",dpqr, sep = ""))
(pdpqr <-  paste("p",dpqr, sep = ""))
(qdpqr <-  paste("q",dpqr, sep = ""))
(rdpqr <-  paste("r",dpqr, sep = ""))
data.frame(cbind(ddpqr,pdpqr, qdpqr, rdpqr), row.names = dpqr) -> data
names(data) <- c("d","p","q","r")
data
print(xtable(data, 
caption = "Density, distribution function, quantile function and random generation for the predefined distributions under R",
label = "dpqr"), 
file = "../tables/dpqrtable.tex")
@
\input{../tables/dpqrtable.tex}
We then define our own probability density function and then use \texttt{fitdistr} from package
\texttt{MASS} to get a maximum likelihood estimate of the parameters:

<<logfitdis,fig=T,eval=T>>=
dmixnor <- function(x, p, m1, sd1, m2, sd2){
  p*dnorm(x, m1, sd1) + (1 - p)*dnorm(x, m2, sd2)
}
library(MASS)
fitdistr(F1, dmixnor, list(p=0.88, m1=-0.04, sd1=0.076, m2=0.34, sd2=0.07))$estimate -> e
e
hist(F1, proba = TRUE, col = grey(0.8), 
main = "Ajustement with a mixture of two normal distributions",
xlab = "First factor for amino-acid variability", las = 1)
xx <- seq(from = min(F1), to = max(F1), length = 200)
lines(xx, dmixnor(xx,e[1],e[2],e[3],e[4],e[5]), lwd = 2)
@

\section{Synonymous and non-synonymous analyses}

Genetic codes are surjective applications from the set codons ($n=64$)
into the set of amino-acids ($n=20$) :

\setkeys{Gin}{width=\textwidth}
<<surjective, echo=F, fig=T, eval=T>>=
#
# Insert 2 (surjective genetic codes)
#
numcode <- 1 # To choose the genetic code

#
# General layout
#
symbols(x = rep(0,3), y = rep(0,3), 
        circles = c(1, 0.75, 0.45), 
        inches = FALSE,
        bg = c("pink", "white", "lightblue"), 
        xlim = c(-1, 1),
        ylim = c(-1, 1),
        bty = "n",
        asp = 1,
        main = paste("The surjective nature of genetic codes\nGenetic code number", numcode),
        xlab = "", ylab= "",
        xaxt ="n", yaxt = "n")
title( sub = "Adapted from insert 2 in Lobry & Chessel (2003) JAG 44:235")

words() -> codons
unlist(lapply(lapply(codons,s2c),translate, numcode = numcode)) -> aa
aaa(aa) -> aa3
#
# sort by alphabetical order of three-letter code of amino-acids
#

neworder <- order(aa3)
aa3 <- aa3[neworder]
aa <- aa[neworder]
codons <- codons[neworder]
#
# Text for codons
#
cangles <- seq(0, 2*pi, le = 65)[1:64]
text(x = sin(cangles)*0.9, y = cos(cangles)*0.9, labels = codons, cex = 0.65)
#
# Text for aa3
#
aangles <- seq(0, 2*pi, le = 22)[1:21]
text(x = sin(aangles)*0.35, y = cos(aangles)*0.35, 
     labels = unique(aa3), cex = 0.8)
#
# Text for aa
#
text(x = sin(aangles)*0.25, y = cos(aangles)*0.25, 
     labels = unique(aa), cex = 0.8)
#
# Draw lines
#
for( i in 1:64 )
{
  target <- aaa(translate(s2c(codons[i]), numcode = numcode))
  n <- which( unique(aa3) == target)
  lines(x = c(sin(cangles[i])*0.85, sin(aangles[n])*0.4), 
        y = c(cos(cangles[i])*0.85, cos(aangles[n])*0.4) )
}
@
\setkeys{Gin}{width=0.8\textwidth}

Two codons encoding the same amino-acid are said synonymous while
two codons encoding a different amino-acid are said non-synonymous.
The distinction between the synonymous and non-synonymous level are
very important in evolutionary studies because most of the selective
pressure is expected to work at the non-synonymous level, because the
amino-acids are the components of the proteins, and therefore more likely
to be subject to selection.

$K_s$ and $K_a$ are an estimation of the number of substitutions per synonymous site 
and per non-synonymous site, respectively, between two protein-coding genes \cite{kaks}.
The $\frac{K_{a}}{K_{s}}$ ratio is used as tool to evaluate selective pressure (see \cite{hurst}
for a nice back to basics). Let's give a simple illustration with three orthologous genes of the 
thioredoxin familiy from \textit{Homo sapiens}, \textit{Mus musculus},
and \textit{Rattus norvegicus} species: 

<<ortho, eval=T>>=
ortho <- read.alignment(system.file("sequences/ortho.fasta", package = "seqinr"), format="fasta")
kaks.ortho <- kaks(ortho)
kaks.ortho$ka/kaks.ortho$ks
@

The  $\frac{K_{a}}{K_{s}}$ ratios are less than $1$, suggesting a selective 
pressure on those proteins during evolution.

For transversal studies (\textit{i.e.} codon usage studies in a genome at the time it was sequenced)
there is little doubt that the strong requirement to distinguish between synonymous and an non-synonymous
variability was the source of many mistakes \cite{misuse}. We have just shown here with a scholarship
example that the metric choice is not neutral. If you consider that the $\chi^{2}$ metric is not too bad,
with respect to your objectives, and that you want to quantify the synonymous and an non-synonymous
variability, please consider reading this paper \cite{lobrychessel}, and follow this link
\url{http://pbil.univ-lyon1.fr/members/lobry/repro/jag03/} for on-line reproducibility.

Let's now use the toy example given in table \ref{toycodon} to illustrate how to study synonymous
and non-synonymous codon usage.

<<toycodon,eval=T>>=
data(toycodon)
toycodon
@

<<xtabletoycodon, fig = FALSE, echo = FALSE,eval=T>>=
print(xtable(toycodon, digits = rep(0,11), caption = "A very simple example of codon counts in three coding sequences to be loaded with \\texttt{data(toycodon).}", label = "toycodon"), 
file = "../tables/toycodon.tex")
@
\input{../tables/toycodon.tex}

Let's first have a look to global codon usage, we do not take into account the structure
of the genetic code:

<<globaltoycodon, fig=T,eval=T>>=
global <- dudi.coa(toycodon, scann = FALSE, nf= 2)
myplot(global, asp = 1, pch = 19, xlab = "", ylab = "", main = "Global codon usage")
@

From a global codon usage point of view, coding sequence number 3 is away.
To take into account the genetic code structure, we need to know for which amino-acid the codons are coding.
The codons are given by the names of the columns of the object \texttt{toycodon}:

<<toy1,fig=F,eval=T>>=
names(toycodon)
@


Put all codon names into a single string:

<<toy2,fig=F,eval=T>>=
c2s(names(toycodon))
@

Transform this string as a vector of characters:

<<toy3,fig=F,eval=T>>=
s2c(c2s(names(toycodon)))
@

Translate this into amino-acids using the default genetic code:

<<toy4,fig=F,eval=T>>=
translate(s2c(c2s(names(toycodon))))
@

Use the three letter code for amino-acid instead:

<<toy5,fig=F,eval=T>>=
aaa(translate(s2c(c2s(names(toycodon)))))
@

Make this a factor:

<<toy5,fig=F,eval=T>>=
facaa <- factor(aaa(translate(s2c(c2s(names(toycodon))))))
facaa
@

The non synonymous codon usage analysis is the between amino-acid analysis:

<<toy6, fig=T,eval=T>>=
nonsynonymous <- t(between(dudi = t(global), fac = facaa, scann = FALSE, nf = 2))
myplot(nonsynonymous, asp = 1, pch = 19, xlab = "", ylab = "", main = "Non synonymous codon usage")
@

This is reminiscent of something, let's have a look at amino-acid counts:

<<toy7,eval=T>>=
by(t(toycodon), facaa, colSums)
@

This is exactly the same data set that we used previously (table \ref{toyaa}) at the amino-acid
level. The non synonymous codon usage analysis is exactly the same as the amino-acid analysis.
Coding sequence number 3 is far away because it codes for many Cys, a rare amino-acid. Note
that at the global codon usage level, this is also the major visible structure. To get rid of this
amino-acid effect, we use the synonymous codon usage analysis, that is the within amino-acid
analysis:

<<toy8, fig=T,eval=T>>=
synonymous <- t(within(dudi = t(global), fac = facaa, scann = FALSE, nf = 2))
myplot(synonymous, asp = 1, pch = 19, xlab = "", ylab = "", main = "Synonymous codon usage")
@

Now, coding sequence number 2 is away. When the amino-acid effect is removed, the pattern is then
completely different. To interpret the result we look at the codon coordinates on the first factor of
synonymous codon usage:

<<toy9,fig=T,eval=T>>=
synonymous$co[ , 1, drop = FALSE] -> tmp
tmp <- tmp[order(tmp$Axis1), , drop = FALSE]
colcod <- sapply(rownames(tmp), function(x) ifelse(substr(x,3,3) == "c" || substr(x,3,3) == "g", "blue", "red"))
pchcod <- ifelse(colcod=="red",1,19)
dotchart(tmp$Axis1, labels = toupper(rownames(tmp)),
color = colcod, pch = pchcod,
main = "Codon coordinates on first factor\nfor synonymous codon usage")
legend("topleft", inset = 0.02, legend = c("GC ending codons", "AT ending codons"),
text.col = c("blue", "red"), pch = c(19,1), col = c("blue","red"), bg = "white")
@

At the synonymous level, coding sequence number 2 is different because it is enriched in GC-ending codons
as compared to the two others. Note that this is hard to see at the global codon usage level because of the
strong amino-acid effect.

\begin{figure}[htbp]
   \begin{center}
      \includegraphics{../figs/lobgau5}
   \end{center}
   \caption{Screenshot of figure 5 from \cite{lobrygautier}. Each point represents
   a protein. This was to show the correlation between the codon adaptation index (CAI Score)
   with the second factor of correspondence analysis at the amino-acid level (F2 Score). Highly
   expressed genes have a high CAI value.
   }
   \label{lobgau5}
\end{figure}


To illustrate the interest of synonymous codon usage analyses, let's use now a more realistic example.
In \cite{lobrygautier} there was an assertion stating that selection for
translation optimisation in \textit{Escherichia coli} was also visible at the amino-acid level.
The argument was in figure 5 of the paper (\textit{cf} fig \ref{lobgau5}), that can be reproduced\footnote{
  the code to reproduce all figures from \cite{lobrygautier} is available at
  \url{http://pbil.univ-lyon1.fr/members/lobry/repro/nar94/}.
} with the following R code:

%
% C'est long celui-la, vaudrait mieux refaire les calculs a partir de ec999, ca illustrerait
% comment on calcule le CAI
%
<<reprolobgau,fig=T, eval=T>>=
ec <- read.table(
     file = "ftp://pbil.univ-lyon1.fr/pub/datasets/NAR94/data.txt",
     header = TRUE,
     row.names = 1)
ec.coa <- dudi.coa(ec, scann = FALSE, nf=3)
F2 <- ec.coa$li[,2]
tmp <- read.table(
     file = "ftp://pbil.univ-lyon1.fr/pub/datasets/NAR94/ecoli999.cai")
cai <- exp(tmp$V2)
if(cor(cai,F2) > 0) F2 <- -F2
plot(cai, F2, pch=20, xlab="CAI Score", ylab="F2 Score",
     main="Fig 5 from Lobry & Gautier (1994) NAR 22:3174")
@


So, there was a correlation between the CAI (Codon Adaptation Index \cite{CAI}) and
the second factor for amino-acid composition variability. However, this
is not completely convincing because the CAI is not completely independent
of the amino-acid composition of the protein. Let's use within amino-acid
correspondence analysis to remove the amino-acid effect. Here is a commented
step-by-step analysis:

<<vlg1, fig = F, eval=T>>=
data(ec999)
class(ec999)
names(ec999)[1:10]
ec999[[1]][1:50]
@

This is to load the data from \cite{lobrygautier} which is available as \texttt{ec999}
in the \seqinr{} package. The letters \texttt{ec} are for the bacterium 
\textit{Escherichia coli} and the number \texttt{999} means that there were
$999$ coding sequences available from this species at that time. The class of the
object \texttt{ec999} is a list, which names are the coding sequence names, for instance
the first coding sequence name is 
\texttt{\Sexpr{ifelse(exists("ec999"), names(ec999)[1], "???")}}.
Each element of the list is a vector of character, we have listed just above the 50 first
character of the first coding sequence of the list with \texttt{ec999[[1]][1:50]}, we
can see that there is a start codon (ATG) at the beginning of the first coding sequence.

<<vlg2, fig = F, eval=T>>=
ec999.uco <- lapply(ec999, uco) # compute codon usage for all CDS
class(ec999.uco)
class(ec999.uco[[1]])
ec999.uco[[1]]
@

This is to compute the codon usage, that is how many times each codon is used
in each coding sequence. Because \texttt{ec999} is a list, we use the function
\texttt{lapply()} to apply the same function, \texttt{uco()}, to all the
elements of the list and we store the result in the object \texttt{ec999.uco}.
The object \texttt{ec999.uco} is a list too, and all its elements belong to
the class table.

<<vlg3, fig = F, eval=T>>=
df <- as.data.frame(lapply(ec999.uco, as.vector)) # put it in a dataframe
dim(df)
df[1:5,1:5]
@

This is to put the codon usage into a data.frame. Note that the codons are in row
and the coding sequences are in columns. This is more convenient for the following
because groups for within and between analyses are usually handled by row.

<<vlg4, fig = F, eval=T>>=
row.names(df) <- names(ec999.uco[[1]]) # add codon names
df[1:5,1:5]
@

This is to keep a trace of codon names, just in case we would like to re-order
the dataframe \texttt{df}. This is important because we can now play with
the data at will without loosing any critical information. 

<<vlg5, fig = F, eval=T>>=
ec999.coa <- dudi.coa(df = df, scannf = FALSE) # run global correspondence analysis
ec999.coa
@

This is to run global correspondence analysis of codon usage.
We have set the \texttt{scannf} parameter to \texttt{FALSE}
because otherwise the eigenvalue bar plot is displayed for the
user to select manually the number of axes to be kept.


<<vlg6, fig = F, eval=T>>=
facaa <- as.factor(aaa(translate(s2c(c2s(rownames(df)))))) # define a factor for amino-acids
facaa
@

This is to define a factor for amino-acids. The function \texttt{translate()} use by
default the standard genetic code and this is OK for \textit{E. coli}.

<<vlg7, fig = F, eval=T>>=
ec999.syn <- within(dudi = ec999.coa, fac = facaa, scannf = FALSE) # run synonymous codon usage analysis
ec999.syn
@

This is to run the synonymous codon usage analysis. The value of the \texttt{ratio} component of
the object \texttt{ec999.syn} shows that most of the variability is at the synonymous
level, a common situation in codon usage studies.

<<vlg8, fig = F, eval=T>>=
ec999.btw <- between(dudi = ec999.coa, fac  = facaa, scannf = FALSE) # run non-sysnonymous codon usage analysis <=> amino-acid usage
ec999.btw
@

This is to run the non-sysnonymous codon usage analysis, or amino-acid usage analysis.

<< vlg9,fig=T,eval=T>>=
x <- ec999.syn$co[,1] 
y <- ec999.btw$co[,2]
if(cor(x,y) < 0) y <- -y
kxy <- kde2d(x,y, n = 100)
nlevels <- 25
breaks <- seq(from = min(kxy$z), to = max(kxy$z), length = nlevels + 1)
col <- cm.colors(nlevels)
image(kxy, breaks = breaks, col = col, xlab = "First synonymous factor",
ylab = "Second non-synonymous factor", xlim = c(-0.5, 0.5),
ylim  = c(-0.3, 0.3), las = 1,
main = "The second factor for amino-acid variability is\ncorrelated with gene expressivity")
contour(kxy, add = TRUE, nlevels = nlevels, drawlabels = FALSE)
box()
abline(c(0,1), lty=2)
abline(lm(y~x))
legend("topleft", lty = c(2,1), legend = c("y = x", "y = lm(y~x)"), inset = 0.01, bg = "white")
@

This is to plot the whole thing. We have extracted the coding sequences coordinates
on the first synonymous factor and the second non-synonymous factor within
\texttt{x} and \texttt{y}, respectively. Because we have many points, we
use the two-dimensional kernel density estimation provided by the function
\texttt{kde2d()} from package \texttt{MASS}.


%
% Akashi Gojobori
%

\textit{To be completed}

<<xaacost, fig = FALSE, echo = FALSE,eval=T>>=
data(aacost)
print(xtable(aacost, digits = rep(0,8), caption = "Aerobic cost of amino-acids in \\textit{Escherichia coli} and G+C classes to be loaded with \\texttt{data(aacost).}", 
label = "aacost"), 
file = "../tables/aacost.tex")
@
\input{../tables/aacost.tex}

\SweaveInput{../config/sessionInfo.rnw}

% END - DO NOT REMOVE THIS LINE

%%%%%%%%%%%%  BIBLIOGRAPHY %%%%%%%%%%%%%%%%%
\clearpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{../config/book}
\end{document}
